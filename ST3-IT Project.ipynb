{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read trainning dataset\n",
    "dataset = \"E:\\\\college\\\\level.4\\\\first term\\\\ST3\\\\Project\\\\offensiveTrainningDataset.tsv\"\n",
    "trainning_data = pd.read_csv(dataset, sep='\\t', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeling dataset\n",
    "tweet = trainning_data[[\"tweet\"]]\n",
    "subtask_a = trainning_data[[\"subtask_a\"]]\n",
    "subtask_b = trainning_data.query(\"subtask_a == 'OFF'\")[[\"subtask_b\"]]\n",
    "subtask_c = trainning_data.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13240\n",
      "13240\n",
      "4400\n",
      "3876\n"
     ]
    }
   ],
   "source": [
    "print(len(tweet))\n",
    "print(len(subtask_a))\n",
    "print(len(subtask_b))\n",
    "print(len(subtask_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "preprocessdTweets = copy.deepcopy(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def removePatterns(tweet):    \n",
    "    patterns = ['URL', '@USER', '\\'ve', 'n\\'t', '\\'s', '\\'m']\n",
    "    for pattern in patterns:\n",
    "        tweet = tweet.replace(pattern, '')\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
    "\n",
    "def removeEmoji(tweet):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',tweet)\n",
    "    \n",
    "def tweetTokenize(tweet):\n",
    "    return word_tokenize(tweet.lower())\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_tokens = []\n",
    "    for token in tokens:  \n",
    "        if token not in stop_words:  \n",
    "            processed_tokens.append(token)  \n",
    "    return processed_tokens\n",
    "\n",
    "def stem_and_lem(tokens):\n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wordnet_lemmatizer.lemmatize(token)\n",
    "        token = lancaster_stemmer.stem(token)\n",
    "        if len(token) > 1:\n",
    "            clean_tokens.append(token)\n",
    "    return clean_tokens\n",
    "\n",
    "##def lemmatizeTokens(tokens):\n",
    "##    lemmatizer = WordNetLemmatizer() \n",
    "##    processed_tokens = []\n",
    "##    for token in tokens:\n",
    "##        token = lemmatizer.lemmatize(token)\n",
    "##        if len(token)>1:\n",
    "##            processed_tokens.append(token)\n",
    "        \n",
    "##def stemmingTokens(tokens):\n",
    "##    processed_tokens = []\n",
    "##    ps = PorterStemmer()\n",
    "##    for token in tokens:\n",
    "##        token = ps.stem(token)\n",
    "##        if len(token)>1:\n",
    "##            processed_tokens.append(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessdTweets['tweet'] = tweet['tweet'].apply(removePatterns)\n",
    "\n",
    "preprocessdTweets['tokens'] = preprocessdTweets['tweet'].apply(removeEmoji)\n",
    "\n",
    "preprocessdTweets['tokens'] = preprocessdTweets['tweet'].apply(tweetTokenize)\n",
    "\n",
    "preprocessdTweets['tokens'] = preprocessdTweets['tokens'].apply(remove_stop_words)\n",
    "\n",
    "preprocessdTweets['tokens'] = preprocessdTweets['tokens'].apply(stem_and_lem)\n",
    "\n",
    "##tqdm.pandas(desc=\"Stemming\")\n",
    "##preprocessdTweets['tokens'] = preprocessdTweets['tokens'].progress_apply(stemmingTokens)\n",
    " \n",
    "text_vector = preprocessdTweets['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1300180\\anaconda3\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tqdm.pandas(desc=\"Stemming and Lemmatizing\")\n",
    "preprocessdTweets['tokens'] = preprocessdTweets['tokens'].apply(stem_and_lem)\n",
    "\n",
    "text_vector = preprocessdTweets['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example before preprocessing\n",
      "                                               tweet\n",
      "0  @USER She should ask a few native Americans wh...\n",
      "\n",
      " this is an example after preprocessing\n",
      "                                               tweet               tokens\n",
      "0   She should ask a few native Americans what th...  [ask, nat, am, tak]\n"
     ]
    }
   ],
   "source": [
    "print(\"this is an example before preprocessing\")\n",
    "print(tweet[:1])\n",
    "\n",
    "print(\"\\n this is an example after preprocessing\")\n",
    "print(preprocessdTweets[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfid(text_vector):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    untokenized_data =[' '.join(tweet) for tweet in tqdm(text_vector, \"Vectorizing...\")]\n",
    "    vectorizer = vectorizer.fit(untokenized_data)\n",
    "    vectors = vectorizer.transform(untokenized_data).toarray()\n",
    "    return vectors\n",
    "\n",
    "def get_vectors(vectors, labels, keyword):\n",
    "    if len(vectors) != len(labels):\n",
    "        print(\"Unmatching sizes!\")\n",
    "        return\n",
    "    result = list()\n",
    "    for vector, label in zip(vectors, labels):\n",
    "        if label == keyword:\n",
    "            result.append(vector)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing...: 100%|██████████| 13240/13240 [00:00<00:00, 156862.40it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_a = tfid(text_vector) # Numerical Vectors A\n",
    "labels_a = subtask_a['subtask_a'].values.tolist() # Subtask A Labels\n",
    "\n",
    "vectors_b = get_vectors(vectors_a, labels_a, \"OFF\") # Numerical Vectors B\n",
    "labels_b = subtask_b['subtask_b'].values.tolist() # Subtask B Labels\n",
    "\n",
    "vectors_c = get_vectors(vectors_b, labels_b, \"TIN\") # Numerical Vectors C\n",
    "labels_c = subtask_c['subtask_c'].values.tolist() # Subtask C Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifing and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def classify(vectors, labels):\n",
    "    # Random Splitting With Ratio 3 : 1\n",
    "    train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors, labels, test_size=0.333)\n",
    "\n",
    "    # Initialize Model\n",
    "    classifier = None\n",
    "    #using LR model\n",
    "    classifier = LogisticRegression(multi_class='auto', solver='newton-cg',)\n",
    "    classifier = GridSearchCV(classifier, {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}, cv=3, n_jobs=4)\n",
    "    classifier.fit(train_vectors, train_labels)\n",
    "    classifier = classifier.best_estimator_\n",
    "\n",
    "    accuracy = accuracy_score(train_labels, classifier.predict(train_vectors))\n",
    "    print(\"Training Accuracy:\", accuracy)\n",
    "    test_predictions = classifier.predict(test_vectors)\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bulding Models A, B, C using Logestic Regrision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Model Subtask A...\n",
      "Training Accuracy: 0.8067036575699241\n",
      "Test Accuracy: 0.7530052166024042\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding Model Subtask A...\")\n",
    "classify(vectors_a[:13240], labels_a[:13240])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Model Subtask B...\n",
      "Training Accuracy: 0.8803680981595092\n",
      "Test Accuracy: 0.8819918144611187\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding Model Subtask B...\")\n",
    "classify(vectors_b[:], labels_b[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Model Subtask C...\n",
      "Training Accuracy: 0.797678916827853\n",
      "Test Accuracy: 0.675445391169636\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding Model Subtask C...\")\n",
    "classify(vectors_c[:], labels_c[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
